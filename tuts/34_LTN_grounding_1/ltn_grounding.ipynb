{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Grounding in Logic Tensor Networks (LTN)\n",
    "\n",
    "## Real Logic\n",
    "\n",
    "The semantics of LTN differs from the standard abstract semantics of First-order Logic (FOL). Specifically, LTN domains are interpreted concretely by tensors in the Real field.\n",
    "\n",
    "To emphasize the fact that LTN interprets symbols which are grounded on real-valued features, we use the term *grounding*, denoted by $\\mathcal{G}$, instead of interpretation.\n",
    "$\\mathcal{G}$ associates a tensor of real numbers to any term of the language, and a real number in the interval $[0,1]$ to any formula $\\phi$.\n",
    "\n",
    "In the rest of the tutorials, we commonly use \"tensor\" to designate \"tensor in the Real field\".\n",
    "\n",
    "Real Logic language consists of a non-logical part (the signature) and logical connectives and quantifiers.\n",
    "* **constants** denote individuals from some space of tensors $\\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$ (tensor of any rank). The individuals can be pre-defined (data point) or learnable (embedding).\n",
    "* **variables** denote sequences of individuals.\n",
    "* **functions** can be any mathematical function either pre-defined or learnable. Examples of functions can be distance functions, regressors, etc. Functions can be defined using any operations in PyTorch. They can be linear functions, Deep Neural Networks, and so forth.\n",
    "* **predicates** are represented as mathematical functions that map from some n-ary domain of individuals to a real number in $[0,1]$, which can be interpreted as a truth degree. Examples of predicates can be similarity measures, classifiers, etc.\n",
    "* **connectives** ($\\lnot, \\land, \\lor, \\implies$) are modeled using fuzzy connective operators.\n",
    "* **quantifiers** are defined using fuzzy aggregators.\n",
    "\n",
    "This tutorial explains how to ground constants, variables, functions, and predicates.\n",
    "\n",
    "In order to properly use LTN, we need to import the framework and PyTorch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import ltn\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constants\n",
    "\n",
    "LTN constants are grounded into real tensors. Each constant $c$ is mapped to a point in $\\mathcal{G}(c) \\in \\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$. Notice that the objects in the domain may be tensors of any rank. For instance, a tensor of rank $0$ corresponds to a scalar, a tensor of rank $1$ to a vector, a tensor of rank $2$ to a matrix and so forth, in the usual way.\n",
    "\n",
    "Here, we define $\\mathcal{G}(c_1)=(2.1,3)$ and $\\mathcal{G}(c_2)=\\begin{pmatrix}\n",
    "4.2 & 3 & 2.5\\\\\n",
    "4 & -1.3 & 1.8\n",
    "\\end{pmatrix}$.\n",
    "\n",
    "Note that the `Constant` constructor needs a PyTorch tensor containing the features of the individual."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([[4.2, 3, 2.5], [4, -1.3, 1.8]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that a constant can be set as learnable by using the optional argument `trainable=True`. This is useful to learn embeddings for some individuals.\n",
    "The features of the tensor will be considered as trainable parameters (learning in LTN is explained in a further notebook)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "c3 = ltn.Constant(torch.tensor([0.,0.]), trainable=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Under the hood, LTN constants are implemented using `LTNObject` objects. These particular objects wrap a value (the individual) and an important attribute called `free_vars`, which will be explained later in the tutorial.\n",
    "\n",
    "If the parameter `trainable` of the LTN constant is set to `True`, then\n",
    "the `requires_grad` parameter of the PyTorch tensor contained in the LTN constant will be set to `True`.\n",
    "\n",
    "The value of an LTN constant can be easily accessed using the `value` attribute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1000, 3.0000])\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(c1.value)\n",
    "print(c3.value)\n",
    "# here, we have to perform a detach before calling numpy(), because the tensor has requires_grad=True\n",
    "print(c3.value.detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicates\n",
    "\n",
    "LTN predicates are functions which map from some n-ary space of input values to a value in [0., 1.]. Predicates in LTN can be neural networks or any other function that achieves such a mapping.\n",
    "\n",
    "There are different ways to construct a predicate in LTN. The constructor `ltn.Predicate(model, func)` permits two types of construction:\n",
    "- if the `model` parameter is not `None`, the predicate is constructed by using the `torch.nn.Module` model instance that has been given as input;\n",
    "- if the `model` parameter is `None` and the `func` parameter is not `None`, the predicate is constructed by using the function given in input. The construction by using\n",
    "a function is suggested for small mathematical operations with **no weight tracking** (non-trainable functions).\n",
    "\n",
    "The following defines a predicate $P_1$ using the `func` parameter and a predicate $P_2$ using the `model` parameter.\n",
    "\n",
    "Notice the access to the `value` attribute for the constant `mu` inside the predicate $P_1$ definition. We need to do that because\n",
    "`x` is a PyTorch tensor, while `mu` an LTN constant. The operand - is not supported for these two different types. In general, every time\n",
    "an LTN object (constant or variable) is used inside the definition of a predicate, we need to access its value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "mu = ltn.Constant(torch.tensor([2., 3.]))\n",
    "P1 = ltn.Predicate(func=lambda x: torch.exp(-torch.norm(x - mu.value, dim=1)))\n",
    "\n",
    "class ModelP2(torch.nn.Module):\n",
    "    \"\"\"For more info on how to use torch.nn.Module:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ModelP2, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dense1 = torch.nn.Linear(2, 5)\n",
    "        self.dense2 = torch.nn.Linear(5, 1) # returns one value in [0,1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.dense1(x))\n",
    "        return self.sigmoid(self.dense2(x))\n",
    "\n",
    "modelP2 = ModelP2()\n",
    "P2 = ltn.Predicate(model=modelP2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can easily query predicates using LTN constants and LTN variables (see further in this notebook to query using variables).\n",
    "\n",
    "Notice the access to the `value` attribute on the evaluation of the predicates. We need to do that because LTN predicates\n",
    "return `LTNObject` instances. Inside the `value` attribute of the returned `LTNObject` there is the actual value of the predicate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9048)\n",
      "tensor(0.0358)\n",
      "tensor(0.3344, grad_fn=<ViewBackward>)\n",
      "<ltn.core.LTNObject object at 0x7fbeb2ca0df0>\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]))\n",
    "c3 = ltn.Constant(torch.tensor([3.0, 4.8]))\n",
    "print(P1(c1).value)\n",
    "print(P1(c2).value)\n",
    "print(P2(c3).value)\n",
    "print(P1(c1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTES:\n",
    "- If an LTN predicate (or an LTN function) takes several inputs, e.g. $P_4(x_1,x_2)$, the arguments must be separated by a comma.\n",
    "- LTN internally converts the inputs such that there is a \"batch\" dimension on the first dim. Therefore, most operations should work with `dim=1` (dimension of the features).\n",
    "\n",
    "Here, we show the implementation of a binary predicate in LTN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1631, grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class ModelP4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelP4, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dense1 = torch.nn.Linear(4, 5)\n",
    "        self.dense2 = torch.nn.Linear(5, 1) # returns one value in [0,1]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.elu(self.dense1(x))\n",
    "        return self.sigmoid(self.dense2(x))\n",
    "\n",
    "P4 = ltn.Predicate(ModelP4())\n",
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]))\n",
    "print(P4(c1, c2).value) # multiple arguments are passed as a list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more details and useful ways to create predicates (incl. how to integrate multiclass classifiers as binary predicates) see the examples included in the /examples folder of this repository."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions\n",
    "\n",
    "LTN functions are grounded as any mathematical function that maps $n$ individuals to one individual in the tensor domain.\n",
    "\n",
    "There are different ways to construct a function in LTN. The constructor `ltn.Function(model, func)` permits two types of construction:\n",
    "- if the `model` parameter is not `None`, the function is constructed by using the `torch.nn.Module` model instance that has been given as input;\n",
    "- if the `model` parameter is `None` and the `func` parameter is not `None`, the function is constructed by using the function given in input. The construction by using\n",
    "a function is suggested for small mathematical operations with **no weight tracking** (non-trainable function).\n",
    "\n",
    "The following defines a function $f_1$ using the `func` parameter, and a function $f_2$ using the `model` parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "f1 = ltn.Function(func=lambda x, y: x - y)\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(2, 10)\n",
    "        self.dense2 = torch.nn.Linear(10, 5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        return self.dense2(x)\n",
    "\n",
    "model_f2 = MyModel()\n",
    "f2 = ltn.Function(model=model_f2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can easily compute functions using LTN constants and LTN variables as inputs (see further in this notebook to query using variables).\n",
    "\n",
    "Notice that the output of an LTN function is an `LTNObject`, as we have already seen for the predicates.\n",
    "\n",
    "The consideration about the necessity of accessing the `value` parameter of the returned `LTNObject` remains the same also for the LTN functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4000,  2.2000])\n",
      "tensor([-0.5990,  0.0462,  0.9553,  0.2285, -0.1920], grad_fn=<ViewBackward>)\n",
      "<ltn.core.LTNObject object at 0x7fbeb2ca0d90>\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]))\n",
    "print(f1(c1, c2).value) # multiple arguments are passed as a list\n",
    "print(f2(c1).value)\n",
    "print(f2(c1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variables\n",
    "\n",
    "LTN variables are sequences of individuals/constants from a domain. Variables are useful in logic to write quantified statements, e.g. $\\forall x\\ P(x)$.\n",
    "\n",
    "Notice that a variable is a sequence and not a set, namely, the same value can occur twice in the sequence.\n",
    "\n",
    "The following defines two variables, $x$ and $y$, with respectively 10 and 5 individuals sampled from normal distributions in $\\mathbb{R}^2$.\n",
    "In LTN, variables need to be labelled (see the arguments `'x'` and `'y'` below). The labels are used internally by LTN to recognize the variables and to\n",
    "implement a lot of other important features.\n",
    "\n",
    "The constructor internally builds an `LTNObject` for the variable. The value will be the given tensor, while the `free_vars` attribute\n",
    "will be the list ['x'] for the first variable, and the list ['y'] for the second variable.\n",
    "\n",
    "The interpretation of the `free_vars` attribute is that it contains the list of the labels of the free variables contained in the `LTNObject`.\n",
    "In logic, a variable is free when it is not quantified by a quantifier, for example the existential quantifier or the universal quantifier.\n",
    "\n",
    "In the case of a variable, it is intuitive to understand that the only free variable is the variable itself. It is for this reason that the `free_vars` attribute\n",
    "contains only the label of the variable itself.\n",
    "\n",
    "For the constants, we did not talk about their `free_vars` attribute since it is an empty list. It is intuitive since a constant\n",
    "is not a variable, so it does not contain free variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "x = ltn.Variable('x', torch.randn((10, 2)))\n",
    "y = ltn.Variable('y', torch.randn((5, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating a term/predicate on one variable of $n$ individuals yields $n$ output values, where the $i$-th output value corresponds to the term calculated with the $i$-th individual.\n",
    "\n",
    "Similarly, evaluating a term/predicate on $k$ variables $(x_1,\\dots,x_k)$, with respectively $n_1,\\dots,n_k$ individuals each, yields a result with $n_1 \\times \\dots \\times n_k$ values. The result is organized in a tensor where the first $k$ dimensions can be indexed to retrieve the outcome(s) that correspond to each variable. The tensor is wrapped in an `LTNObject` where the attribute `free_vars` tells which dim of the tensor corresponds to which variable (using the label of the variable).\n",
    "\n",
    "In the example below, the result of $P_4$ has shape 10x5 since x has 10 individuals and y 5 individuals.\n",
    "The first cell of the result is the evaluation of $P_4$ on the first individual of x and first individual of y, the second\n",
    "cell is the evaluation of $P_4$ on the first individual of x and second individual of y, and so forth.\n",
    "\n",
    "Notice the use of the method `shape()` invoked on the `LTNObject` representing the evaluation of $P_4$. This method is a\n",
    "short-cut for `res1.value.shape`.\n",
    "\n",
    "The attribute `free_vars` contains in this case x and y. This is intuitive since no quantifier has been applied.\n",
    "\n",
    "As shown on the last prints, it is possible to access single cells of the tensor containing the result, or the entire result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "['x', 'y']\n",
      "tensor(0.5214, grad_fn=<SelectBackward>)\n",
      "tensor([[0.4613, 0.4554, 0.4371, 0.4222, 0.4337],\n",
      "        [0.5605, 0.5518, 0.5022, 0.4732, 0.4861],\n",
      "        [0.5214, 0.5183, 0.4742, 0.4522, 0.4628],\n",
      "        [0.6544, 0.6495, 0.5967, 0.5687, 0.5667],\n",
      "        [0.6029, 0.5915, 0.5419, 0.5071, 0.5168],\n",
      "        [0.5663, 0.5653, 0.5084, 0.4846, 0.4832],\n",
      "        [0.5665, 0.5623, 0.5084, 0.4821, 0.4863],\n",
      "        [0.5778, 0.5717, 0.5191, 0.4899, 0.4956],\n",
      "        [0.5978, 0.5867, 0.5368, 0.5023, 0.5126],\n",
      "        [0.5035, 0.5013, 0.4635, 0.4439, 0.4547]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Notice that the outcome is a 2-dimensional tensor where each cell\n",
    "# represents the satisfiability of P4 evaluated with each individual in x and in y.\n",
    "P4 = ltn.Predicate(ModelP4())\n",
    "res1 = P4(x, y)\n",
    "print(res1.shape())\n",
    "print(res1.free_vars) # dynamically added attribute; tells that axis 0 corresponds to x and axis 1 to y\n",
    "print(res1.value[2, 0]) # gives the result computed with the 3rd individual in x and the 1st individual in y\n",
    "print(res1.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we see the same example but applied to an LTN function instead of an LTN predicate.\n",
    "\n",
    "Notice that the output is no more in [0., 1.] since we have now applied a function.\n",
    "\n",
    "Notice also the changes on the shape. In the case of the predicate we had shape 10x5, since the\n",
    "result contained 10x5 truth values in [0., 1.]. Now, the function does not return a scalar. Instead,\n",
    "it returns a real vector in $\\mathbb{R}^2$. This explains the new shape 10x5x2. For each combination of the two variables\n",
    "we have now a vector in $\\mathbb{R}^2$ returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 2])\n",
      "['x', 'y']\n",
      "tensor([-1.5968, -0.1742])\n",
      "tensor([[[-3.6696, -0.8967],\n",
      "         [-2.7439, -1.3845],\n",
      "         [-2.2959, -0.0168],\n",
      "         [-1.1128,  0.0048],\n",
      "         [-1.9483,  0.5959]],\n",
      "\n",
      "        [[-1.5144, -0.8138],\n",
      "         [-0.5887, -1.3016],\n",
      "         [-0.1407,  0.0661],\n",
      "         [ 1.0424,  0.0877],\n",
      "         [ 0.2069,  0.6788]],\n",
      "\n",
      "        [[-1.5968, -0.1742],\n",
      "         [-0.6712, -0.6620],\n",
      "         [-0.2232,  0.7057],\n",
      "         [ 0.9599,  0.7273],\n",
      "         [ 0.1244,  1.3184]],\n",
      "\n",
      "        [[ 0.5213, -0.9353],\n",
      "         [ 1.4469, -1.4232],\n",
      "         [ 1.8950, -0.0555],\n",
      "         [ 3.0781, -0.0339],\n",
      "         [ 2.2426,  0.5572]],\n",
      "\n",
      "        [[-1.2380, -1.4488],\n",
      "         [-0.3124, -1.9366],\n",
      "         [ 0.1356, -0.5689],\n",
      "         [ 1.3187, -0.5473],\n",
      "         [ 0.4832,  0.0438]],\n",
      "\n",
      "        [[-0.2912,  0.0918],\n",
      "         [ 0.6344, -0.3960],\n",
      "         [ 1.0824,  0.9717],\n",
      "         [ 2.2655,  0.9932],\n",
      "         [ 1.4300,  1.5843]],\n",
      "\n",
      "        [[-0.6577, -0.1816],\n",
      "         [ 0.2680, -0.6694],\n",
      "         [ 0.7160,  0.6982],\n",
      "         [ 1.8991,  0.7198],\n",
      "         [ 1.0636,  1.3109]],\n",
      "\n",
      "        [[-0.7375, -0.4544],\n",
      "         [ 0.1882, -0.9422],\n",
      "         [ 0.6362,  0.4255],\n",
      "         [ 1.8193,  0.4471],\n",
      "         [ 0.9838,  1.0382]],\n",
      "\n",
      "        [[-1.2662, -1.3628],\n",
      "         [-0.3406, -1.8506],\n",
      "         [ 0.1074, -0.4830],\n",
      "         [ 1.2905, -0.4614],\n",
      "         [ 0.4550,  0.1297]],\n",
      "\n",
      "        [[-2.0947, -0.3081],\n",
      "         [-1.1691, -0.7959],\n",
      "         [-0.7211,  0.5718],\n",
      "         [ 0.4620,  0.5934],\n",
      "         [-0.3735,  1.1845]]])\n"
     ]
    }
   ],
   "source": [
    "# Notice that the last axe(s) correspond to the dimensions of the outcome;\n",
    "# here, f2 projects to outcomes in R^2, so the outcome has one additional axis of dimension 2.\n",
    "# the output tensor has shape (10, 5, 2) because variable x has 10 individuals, y has 5 individuals, and f1 maps in R^2\n",
    "res2 = f1(x, y)\n",
    "print(res2.shape())\n",
    "print(res2.free_vars)\n",
    "print(res2.value[2,0]) # gives the result calculated with the 3rd individual in x and the 1st individual in y\n",
    "print(res2.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this last example, we apply a predicate to a variable and a constant.\n",
    "\n",
    "It is interesting to notice that the `free_vars` attribute contains now only y, since only variable y has been given in input.\n",
    "\n",
    "Also, the shape is changed. Now, it is a vector of 5 values. The first cell contains the evaluation of the predicate on\n",
    "$c_1$ and the first individual of $y$, the second cell contains the evaluation of the predicate on $c_1$ and the second individual of $y$, and so forth.\n",
    "Since $y$ has 5 individuals in total, the result is intuitively a vector containing 5 truth values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "['y']\n",
      "tensor(0.4494, grad_fn=<SelectBackward>)\n",
      "tensor([0.4494, 0.4598, 0.3997, 0.3894, 0.3824], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]))\n",
    "res3 = P4(c1, y)\n",
    "print(res3.shape()) # Notice that no axis is associated to a constant. The output has shape (5,) because variable y has\n",
    "# 5 individuals\n",
    "print(res3.free_vars)\n",
    "print(res3.value[0]) # gives the result calculated with c1 and the 1st individual in y\n",
    "print(res3.value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables made of trainable constants\n",
    "\n",
    "In LTN, it is possible to define variables which individuals are trainable constants. This is really useful in embedding learning\n",
    "tasks, where the individuals of a variable could be embeddings that have to be learned.\n",
    "\n",
    "In order to better understand how trainable variables can be used in LTN, it is kindly suggested to carefully read the `Smokes-friends-cancer` example, where variables\n",
    "made of trainable constants are used.\n",
    "\n",
    "In this example, two trainable constants are declared. Then, an LTN variable is built using these constants.\n",
    "The first individual of the variable will be the first constant, while the second individual will be the second constant.\n",
    "\n",
    "Notice that after $P_2$ has been evaluated on $x$, both individuals of $x$ will have a `grad_fn` attribute. This means the gradient\n",
    "has passed through the learnable constants by using the predicate.\n",
    "\n",
    "It is important to note that in the declaration of the variables we have given the values of the constants and not the constants themselves.\n",
    "This is why the two constants have `grad_fn` set to `None` after the predicate is applied. Since we have given the value, and\n",
    "not the constant itself, the constants will remain untouched after the application of the predicate. Specifically, we have given the values of the constants to the variable since\n",
    "LTN variables only accepts PyTorch tensors as values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3727, 0.4806], grad_fn=<ViewBackward>)\n",
      "tensor([2.1000, 3.0000], grad_fn=<SelectBackward>)\n",
      "tensor([4.5000, 0.8000], grad_fn=<SelectBackward>)\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.Constant(torch.tensor([2.1, 3]), trainable=True)\n",
    "c2 = ltn.Constant(torch.tensor([4.5, 0.8]), trainable=True)\n",
    "\n",
    "# PyTorch will keep track of the gradients between c1, c2 and x.\n",
    "# Read tutorial 3 for more details.\n",
    "x = ltn.Variable('x', torch.stack([c1.value, c2.value]))\n",
    "res = P2(x)\n",
    "print(res.value)\n",
    "\n",
    "print(x.value[0])\n",
    "print(x.value[1])\n",
    "\n",
    "print(c1.value.grad_fn)\n",
    "print(c2.value.grad_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
